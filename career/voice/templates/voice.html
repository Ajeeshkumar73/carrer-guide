<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Voice Assistant</title>
    <style>
        /* Customization: Simple CSS for a clean chat look */
        body { font-family: sans-serif; display: flex; flex-direction: column; align-items: center; background-color: #f0f2f5; }
        #container { width: 90%; max-width: 600px; background: white; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 20px; margin-top: 50px; }
        #chat-output { min-height: 200px; max-height: 400px; overflow-y: auto; border: 1px solid #ccc; padding: 10px; margin-bottom: 20px; border-radius: 4px; }
        .message { margin-bottom: 10px; padding: 8px; border-radius: 6px; }
        .user-msg { background-color: #007bff; color: white; text-align: right; margin-left: 20%; }
        .assistant-msg { background-color: #e9ecef; color: #333; text-align: left; margin-right: 20%; }
        #microphone-btn { font-size: 24px; padding: 15px 30px; border: none; border-radius: 50px; cursor: pointer; background-color: #4CAF50; color: white; transition: background-color 0.3s; }
        #microphone-btn:hover { background-color: #45a049; }
        .listening { background-color: #dc3545 !important; }
        .listening::after { content: " | LISTENING..."; }
    </style>
</head>
<body>
    <div id="container">
        <h1>Custom Gemini Voice Assistant</h1>
        <div id="chat-output">
            <div class="message assistant-msg">Hello! Click the microphone to start speaking.</div>
        </div>
        <p id="status-message">Ready.</p>
        <button id="microphone-btn">
            üéôÔ∏è Speak to Assistant
        </button>
    </div>
    <script>
        const micBtn = document.getElementById('microphone-btn');
const chatOutput = document.getElementById('chat-output');
const statusMessage = document.getElementById('status-message');
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
const SpeechSynthesis = window.speechSynthesis;

let isListening = false;
let recognition;

// --- Helper Functions ---

function displayMessage(text, sender) {
    const div = document.createElement('div');
    div.className = `message ${sender === 'user' ? 'user-msg' : 'assistant-msg'}`;
    div.textContent = text;
    chatOutput.appendChild(div);
    chatOutput.scrollTop = chatOutput.scrollHeight; // Auto-scroll to bottom
}

function speak(text) {
    if (SpeechSynthesis) {
        const utterance = new SpeechSynthesisUtterance(text);
        // Customization: You can set voice, pitch, and rate here
        // utterance.voice = SpeechSynthesis.getVoices().find(v => v.name === 'Google UK English Female');
        // utterance.rate = 1.1; 
        SpeechSynthesis.speak(utterance);
    } else {
        console.warn("Text-to-Speech not supported in this browser.");
    }
}

async function sendTextToDjango(text) {
    statusMessage.textContent = "Assistant is thinking...";

    // CSRF Token for security (important in production Django)
    const csrfToken = document.querySelector('[name=csrfmiddlewaretoken]')?.value;

    try {
        const response = await fetch('/chat/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': csrfToken, 
            },
            body: JSON.stringify({ message: text }),
        });

        const data = await response.json();
        const reply = data.reply || "Sorry, I encountered an error.";

        displayMessage(reply, 'assistant');
        speak(reply);

    } catch (error) {
        console.error('Error sending message to backend:', error);
        const errorMsg = "Network error. Please try again.";
        displayMessage(errorMsg, 'assistant');
        speak(errorMsg);
    } finally {
        statusMessage.textContent = "Ready.";
    }
}


// --- Voice Assistant Core Logic ---

if (SpeechRecognition) {
    recognition = new SpeechRecognition();
    recognition.continuous = false; // Only one command per press
    recognition.lang = 'en-US';

    recognition.onstart = () => {
        isListening = true;
        micBtn.classList.add('listening');
        micBtn.textContent = 'üî¥ Speaking...';
        statusMessage.textContent = "Listening for your command...";
    };

    recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript;
        displayMessage(transcript, 'user');
        sendTextToDjango(transcript); // Send to Django backend
    };

    recognition.onend = () => {
        isListening = false;
        micBtn.classList.remove('listening');
        micBtn.textContent = 'üéôÔ∏è Speak to Assistant';
        if (statusMessage.textContent.includes("Listening")) {
            statusMessage.textContent = "Ready.";
        }
    };

    recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
        statusMessage.textContent = `Error: ${event.error}. Please try again.`;
        micBtn.classList.remove('listening');
        micBtn.textContent = 'üéôÔ∏è Speak to Assistant';
    };

} else {
    micBtn.disabled = true;
    statusMessage.textContent = "Voice Recognition not supported in your browser.";
}

// --- Event Listener ---
micBtn.addEventListener('click', () => {
    if (!isListening) {
        try {
            recognition.start();
        } catch (e) {
            console.warn('Recognition already started or error:', e);
        }
    } else {
        recognition.stop();
    }
});

// Optional: Pre-fetch voices for better TTS performance
if (SpeechSynthesis) {
    SpeechSynthesis.getVoices();
}
    </script>
</body>
</html>